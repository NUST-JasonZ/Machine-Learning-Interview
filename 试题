1.为什么说Dropout可以解决过拟合？
答：起到取平均的作用：dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于
    对很多个不同的神经网络取平均）
    减少神经元之间复杂的共适应关系：权值的更新不再依赖于有固定关系的隐含节点的共同作用，迫使网络去学习更加鲁棒的特征，换句话说假如我们的神经
    网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。
    
2.Dropout机制？
答：训练时：在训练网络的每个单元都要添加一道概率流程，使用Bernoulli函数生成概率r向量，也就是随机生成一个0、1的向量。代码层面实现让某个神经元
    以概率p停止工作，其实就是让它的激活函数值以概率p变为0。
    测试时：测试时为了让预测概率得到确定值，就不会使用概率忽略的方式，而是使用所有神经元，每一个神经单元的权重参数要乘以概率p，即P*W。
 
3.有哪些优化器，特点和作用？
答：一般优化器都是基于SGD（随机梯度下降），有动量(Momentum)、AdaGrad（自适应梯度）、RMSProp（Root Mean Square Prop）和adam等
    Momentum优化器的主要思想就是利用了类似于移动指数加权平均的方法来对网络的参数进行平滑处理的，让梯度的摆动幅度变得更小。可以解决mini-batch SGD
    优化算法更新幅度摆动大的问题，同时可以使得网络的收敛速度更快。
    AdaGrad（自适应梯度）增加了分母（梯度平方累积和的平方根），能更好利用稀疏梯度的信息，
    RMSprop为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重 W 和偏置 b 的梯度使用了微分平方
    加权平均数。
    Adam（Adaptive Moment Estimation）算法是将Momentum算法和RMSProp算法结合起来使用的一种算法，
4.类域界面方程法中，不能求线性不可分情况下分类问题近似或精确解的方法是？
答：伪逆法：径向基（RBF）神经网络的训练算法，径向基解决的就是线性不可分的情况。
    感知器算法：线性分类模型。
    H-K算法：在最小均方误差准则下求得权矢量，二次准则解决非线性问题。
    势函数法：势函数非线性。
5.线性分类器准则？
答：线性分类器有三大类：感知器准则函数、SVM、Fisher准则
    感知器准则函数：代价函数J=-(W*X+w0)，分类的准则是最小化代价函数。感知器是神经网络（NN）的基础，网上有很多介绍。
    SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。
        （使用核函数可解决非线性问题）
    Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，
        不同类样本距离尽可能大，具体为最大化“广义瑞利商”。
