1.为什么说Dropout可以解决过拟合？
答：起到取平均的作用：dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于
    对很多个不同的神经网络取平均）
    减少神经元之间复杂的共适应关系：权值的更新不再依赖于有固定关系的隐含节点的共同作用，迫使网络去学习更加鲁棒的特征，换句话说假如我们的神经
    网络是在做出某种预测，它不应该对一些特定的线索片段太过敏感，即使丢失特定的线索，它也应该可以从众多其它线索中学习一些共同的特征。
    
2.Dropout机制？
答：训练时：在训练网络的每个单元都要添加一道概率流程，使用Bernoulli函数生成概率r向量，也就是随机生成一个0、1的向量。代码层面实现让某个神经元
    以概率p停止工作，其实就是让它的激活函数值以概率p变为0。
    测试时：测试时为了让预测概率得到确定值，就不会使用概率忽略的方式，而是使用所有神经元，每一个神经单元的权重参数要乘以概率p，即P*W。
 
3.有哪些优化器，特点和作用？
答：一般优化器都是基于SGD（随机梯度下降），有动量(Momentum)、AdaGrad（自适应梯度）、RMSProp（Root Mean Square Prop）和adam等
    Momentum优化器的主要思想就是利用了类似于移动指数加权平均的方法来对网络的参数进行平滑处理的，让梯度的摆动幅度变得更小。可以解决mini-batch SGD
    优化算法更新幅度摆动大的问题，同时可以使得网络的收敛速度更快。
    AdaGrad（自适应梯度）增加了分母（梯度平方累积和的平方根），能更好利用稀疏梯度的信息，
    RMSprop为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重 W 和偏置 b 的梯度使用了微分平方
    加权平均数。
    Adam（Adaptive Moment Estimation）算法是将Momentum算法和RMSProp算法结合起来使用的一种算法，
